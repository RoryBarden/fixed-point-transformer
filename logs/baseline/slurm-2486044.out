/var/spool/slurmd.spool/job2486044/slurm_script: line 14: cuDNN/8.0.4.30-CUDA-11.1.1: No such file or directory
| NOTE: you may get better performance with: --ddp-backend=no_c10d
| distributed init (rank 0): tcp://localhost:14777
| distributed init (rank 1): tcp://localhost:14777
| distributed init (rank 2): tcp://localhost:14777
| distributed init (rank 3): tcp://localhost:14777
| initialized host gpu12.pri.stanage.alces.network as rank 0
| initialized host gpu12.pri.stanage.alces.network as rank 2
| initialized host gpu12.pri.stanage.alces.network as rank 1
| initialized host gpu12.pri.stanage.alces.network as rank 3
/users/ace19rb/.conda/envs/share_layer_params_baseline/lib/python3.8/site-packages/torch/nn/parallel/distributed.py:425: UserWarning: The `check_reduction` argument in `DistributedDataParallel` module is deprecated. Please avoid using it.
  warnings.warn(
/users/ace19rb/.conda/envs/share_layer_params_baseline/lib/python3.8/site-packages/torch/nn/parallel/distributed.py:425: UserWarning: The `check_reduction` argument in `DistributedDataParallel` module is deprecated. Please avoid using it.
  warnings.warn(
/users/ace19rb/.conda/envs/share_layer_params_baseline/lib/python3.8/site-packages/torch/nn/parallel/distributed.py:425: UserWarning: The `check_reduction` argument in `DistributedDataParallel` module is deprecated. Please avoid using it.
  warnings.warn(
Namespace(activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, arch='transformer_wmt_en_de', attention_dropout=0.0, best_checkpoint_metric='loss', bpe=None, bucket_cap_mb=25, clip_norm=0.0, cpu=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='data-bin/wmt16_en_de_bpe32k', dataset_impl=None, ddp_backend='c10d', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layers=12, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method='tcp://localhost:14777', distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=4, dropout=0.3, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=2048, encoder_layers=12, encoder_learned_pos=False, encoder_normalize_before=False, fast_stat_sync=False, find_unused_parameters=False, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=False, fp16_init_scale=128, fp16_scale_tolerance=0.0, fp16_scale_window=None, gradient_as_delta=False, init_type='adaptive', keep_interval_updates=-1, keep_last_epochs=20, label_smoothing=0.1, layer_wise_attention=False, lazy_load=False, left_pad_source='True', left_pad_target='False', load_alignments=False, log_format=None, log_interval=100, lr=[0.002], lr_scheduler='inverse_sqrt', max_epoch=0, max_sentences=None, max_sentences_valid=None, max_source_positions=1024, max_target_positions=1024, max_tokens=3584, max_tokens_valid=3584, max_update=50000, maximize_best_checkpoint_metric=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=1e-09, mixed_precision=False, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_token_positional_embeddings=False, num_workers=1, optimizer='adam', optimizer_overrides='{}', plot_gradient=False, plot_stability=False, plot_variance=False, raw_text=False, required_batch_size_multiple=8, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='model-save-dir', save_interval=1, save_interval_updates=0, seed=1, sentence_avg=False, share_all_embeddings=True, share_decoder_input_output_embed=False, share_layer_num=2, share_params_cross_layer=True, share_type='cycle_reverse', skip_invalid_size_inputs_valid_test=False, source_lang=None, target_lang=None, task='translation', tbmf_wrapper=False, tensorboard_logdir='', threshold_loss_scale=None, tokenizer=None, train_subset='train', update_freq=[32], upsample_primary=1, use_bmuf=False, user_dir=None, valid_subset='valid', validate_interval=1, warmup_init_lr=1e-07, warmup_updates=8000, weight_decay=0.0)
| [en] dictionary: 14568 types
| [de] dictionary: 14568 types
| loaded 3000 examples from: data-bin/wmt16_en_de_bpe32k/valid.en-de.en
| loaded 3000 examples from: data-bin/wmt16_en_de_bpe32k/valid.en-de.de
| data-bin/wmt16_en_de_bpe32k valid en-de 3000 examples
source len: 1024
target len: 1024
1
layer_num: 0, layer_iter: 1.0
encoder attn ratio: 1.0
layer_num: 0, layer_iter: 2.0
encoder ffn ratio: 1.520343542098999
layer_num: 1, layer_iter: 3.0
encoder attn ratio: 1.6629321575164795
layer_num: 1, layer_iter: 4.0
encoder ffn ratio: 1.7166023254394531
layer_num: 2, layer_iter: 5.0
encoder attn ratio: 1.8478870391845703
layer_num: 2, layer_iter: 6.0
encoder ffn ratio: 1.8941705226898193
layer_num: 3, layer_iter: 7.0
encoder attn ratio: 2.0070347785949707
layer_num: 3, layer_iter: 8.0
encoder ffn ratio: 2.059675931930542
layer_num: 4, layer_iter: 9.0
encoder attn ratio: 2.1679866313934326
layer_num: 4, layer_iter: 10.0
encoder ffn ratio: 2.2195186614990234
layer_num: 5, layer_iter: 11.0
encoder attn ratio: 2.3206090927124023
layer_num: 5, layer_iter: 12.0
encoder ffn ratio: 2.382983684539795
layer_num: 6, layer_iter: 13.0
encoder attn ratio: 2.47210955619812
layer_num: 6, layer_iter: 14.0
encoder ffn ratio: 2.5312235355377197
layer_num: 7, layer_iter: 15.0
encoder attn ratio: 2.6138827800750732
layer_num: 7, layer_iter: 16.0
encoder ffn ratio: 2.668633460998535
layer_num: 8, layer_iter: 17.0
encoder attn ratio: 2.7522125244140625
layer_num: 8, layer_iter: 18.0
encoder ffn ratio: 2.807097911834717
layer_num: 9, layer_iter: 19.0
encoder attn ratio: 2.895045518875122
layer_num: 9, layer_iter: 20.0
encoder ffn ratio: 2.9540839195251465
layer_num: 10, layer_iter: 21.0
encoder attn ratio: 3.02681303024292
layer_num: 10, layer_iter: 22.0
encoder ffn ratio: 3.087460994720459
layer_num: 11, layer_iter: 23.0
encoder attn ratio: 3.162876844406128
layer_num: 11, layer_iter: 24.0
encoder ffn ratio: 3.212754726409912
layer_num: 0, layer_iter: 1.0
decoder self ratio: 1.0
layer_num: 0, layer_iter: 2.0
decoder en ratio: 1.585978627204895
layer_num: 0, layer_iter: 3.0
decoder ffn ratio: 1.6935501098632812
layer_num: 1, layer_iter: 4.0
decoder self ratio: 1.826002836227417
layer_num: 1, layer_iter: 5.0
decoder en ratio: 1.899922251701355
layer_num: 1, layer_iter: 6.0
decoder ffn ratio: 1.988216519355774
layer_num: 2, layer_iter: 7.0
decoder self ratio: 2.0975284576416016
layer_num: 2, layer_iter: 8.0
decoder en ratio: 2.167722702026367
layer_num: 2, layer_iter: 9.0
decoder ffn ratio: 2.240804672241211
layer_num: 3, layer_iter: 10.0
decoder self ratio: 2.3377346992492676
layer_num: 3, layer_iter: 11.0
decoder en ratio: 2.4095747470855713
layer_num: 3, layer_iter: 12.0
decoder ffn ratio: 2.4802281856536865
layer_num: 4, layer_iter: 13.0
decoder self ratio: 2.5701448917388916
layer_num: 4, layer_iter: 14.0
decoder en ratio: 2.6325747966766357
layer_num: 4, layer_iter: 15.0
decoder ffn ratio: 2.6998579502105713
layer_num: 5, layer_iter: 16.0
decoder self ratio: 2.780452251434326
layer_num: 5, layer_iter: 17.0
decoder en ratio: 2.8450872898101807
layer_num: 5, layer_iter: 18.0
decoder ffn ratio: 2.9128570556640625
layer_num: 6, layer_iter: 19.0
decoder self ratio: 2.986426591873169
layer_num: 6, layer_iter: 20.0
decoder en ratio: 3.0457983016967773
layer_num: 6, layer_iter: 21.0
decoder ffn ratio: 3.1094236373901367
layer_num: 7, layer_iter: 22.0
decoder self ratio: 3.1770684719085693
layer_num: 7, layer_iter: 23.0
decoder en ratio: 3.2428548336029053
layer_num: 7, layer_iter: 24.0
decoder ffn ratio: 3.297701597213745
layer_num: 8, layer_iter: 25.0
decoder self ratio: 3.3642964363098145
layer_num: 8, layer_iter: 26.0
decoder en ratio: 3.4305901527404785
layer_num: 8, layer_iter: 27.0
decoder ffn ratio: 3.4806103706359863
layer_num: 9, layer_iter: 28.0
decoder self ratio: 3.5458500385284424
layer_num: 9, layer_iter: 29.0
decoder en ratio: 3.6123085021972656
layer_num: 9, layer_iter: 30.0
decoder ffn ratio: 3.6567907333374023
layer_num: 10, layer_iter: 31.0
decoder self ratio: 3.7180001735687256
layer_num: 10, layer_iter: 32.0
decoder en ratio: 3.776127338409424
layer_num: 10, layer_iter: 33.0
decoder ffn ratio: 3.821091413497925
layer_num: 11, layer_iter: 34.0
decoder self ratio: 3.876079797744751
layer_num: 11, layer_iter: 35.0
decoder en ratio: 3.9286458492279053
layer_num: 11, layer_iter: 36.0
decoder ffn ratio: 3.9733970165252686
TransformerModel(
  (encoder): TransformerEncoder(
    (embed_tokens): Embedding(14568, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (6): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (7): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (8): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (9): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (10): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (11): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (embed_tokens): Embedding(14568, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (6): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (7): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (8): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (9): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (10): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (11): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
)
| model transformer_wmt_en_de, criterion LabelSmoothedCrossEntropyCriterion
| num. model params: 51628032 (num. trained: 51628032)
| training on 4 GPUs
| max tokens per GPU = 3584 and max sentences per GPU = None
| no existing checkpoint found model-save-dir/checkpoint_last.pt
| loading train data for epoch 0
| loaded 4500737 examples from: data-bin/wmt16_en_de_bpe32k/train.en-de.en
| loaded 4500737 examples from: data-bin/wmt16_en_de_bpe32k/train.en-de.de
| data-bin/wmt16_en_de_bpe32k train en-de 4500737 examples
| NOTICE: your device may support faster training with --fp16
/users/ace19rb/.conda/envs/share_layer_params_baseline/lib/python3.8/site-packages/torch/nn/parallel/distributed.py:425: UserWarning: The `check_reduction` argument in `DistributedDataParallel` module is deprecated. Please avoid using it.
  warnings.warn(
| epoch 001:    100 / 413 loss=13.180, nll_loss=13.026, ppl=8342.43, wps=160748, ups=0, wpb=404334.802, bsz=10925.307, num_updates=101, lr=2.53487e-05, gnorm=1.388, clip=0.000, oom=0.000, wall=279, train_wall=238
resetting loss stats
| epoch 001:    200 / 413 loss=11.499, nll_loss=11.132, ppl=2244.86, wps=160970, ups=0, wpb=403773.590, bsz=10918.470, num_updates=201, lr=5.03475e-05, gnorm=0.527, clip=0.000, oom=0.000, wall=251, train_wall=234
resetting loss stats
| epoch 001:    300 / 413 loss=10.838, nll_loss=10.325, ppl=1282.87, wps=162728, ups=0, wpb=404897.360, bsz=10869.860, num_updates=301, lr=7.53462e-05, gnorm=0.528, clip=0.000, oom=0.000, wall=249, train_wall=235
resetting loss stats
| epoch 001:    400 / 413 loss=10.258, nll_loss=9.641, ppl=798.36, wps=162980, ups=0, wpb=404169.290, bsz=10911.280, num_updates=401, lr=0.000100345, gnorm=0.650, clip=0.000, oom=0.000, wall=248, train_wall=236
resetting loss stats
| epoch 001 | loss 9.833 | nll_loss 9.144 | ppl 565.92 | wps 160206 | ups 0 | wpb 397937.667 | bsz 10610.000 | num_updates 413 | lr 0.000103345 | gnorm 0.592 | clip 0.000 | oom 0.000 | wall 30 | train_wall 28
/users/ace19rb/Desktop/COM4520/share_layer_params/fairseq/modules/multihead_attention.py:156: UserWarning: Output 0 of SplitBackward is a view and is being modified inplace. This view is an output of a function that returns multiple views. Inplace operators on such views are being deprecated and will be forbidden starting from version 1.8. Consider using `unsafe_` version of the function that produced this view or don't modify this view inplace. (Triggered internally at  /pytorch/torch/csrc/autograd/variable.cpp:547.)
  q *= self.scaling
/users/ace19rb/Desktop/COM4520/share_layer_params/fairseq/optim/adam.py:160: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:1005.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
/users/ace19rb/Desktop/COM4520/share_layer_params/fairseq/modules/multihead_attention.py:156: UserWarning: Output 0 of SplitBackward is a view and is being modified inplace. This view is an output of a function that returns multiple views. Inplace operators on such views are being deprecated and will be forbidden starting from version 1.8. Consider using `unsafe_` version of the function that produced this view or don't modify this view inplace. (Triggered internally at  /pytorch/torch/csrc/autograd/variable.cpp:547.)
  q *= self.scaling
/users/ace19rb/Desktop/COM4520/share_layer_params/fairseq/optim/adam.py:160: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:1005.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
/users/ace19rb/Desktop/COM4520/share_layer_params/fairseq/modules/multihead_attention.py:156: UserWarning: Output 0 of SplitBackward is a view and is being modified inplace. This view is an output of a function that returns multiple views. Inplace operators on such views are being deprecated and will be forbidden starting from version 1.8. Consider using `unsafe_` version of the function that produced this view or don't modify this view inplace. (Triggered internally at  /pytorch/torch/csrc/autograd/variable.cpp:547.)
  q *= self.scaling
/users/ace19rb/Desktop/COM4520/share_layer_params/fairseq/optim/adam.py:160: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:1005.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
/users/ace19rb/Desktop/COM4520/share_layer_params/fairseq/modules/multihead_attention.py:156: UserWarning: Output 0 of SplitBackward is a view and is being modified inplace. This view is an output of a function that returns multiple views. Inplace operators on such views are being deprecated and will be forbidden starting from version 1.8. Consider using `unsafe_` version of the function that produced this view or don't modify this view inplace. (Triggered internally at  /pytorch/torch/csrc/autograd/variable.cpp:547.)
  q *= self.scaling
/users/ace19rb/Desktop/COM4520/share_layer_params/fairseq/optim/adam.py:160: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:1005.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
| epoch 001 | valid on 'valid' subset | loss 9.654 | nll_loss 8.922 | ppl 484.93 | num_updates 413
| saved checkpoint model-save-dir/checkpoint1.pt (epoch 1 @ 413 updates) (writing took 4.795952796936035 seconds)
| epoch 002:    100 / 413 loss=9.455, nll_loss=8.702, ppl=416.45, wps=161023, ups=0, wpb=404511.941, bsz=11037.149, num_updates=514, lr=0.000128594, gnorm=0.642, clip=0.000, oom=0.000, wall=291, train_wall=265
resetting loss stats
| epoch 002:    200 / 413 loss=8.815, nll_loss=7.961, ppl=249.23, wps=161158, ups=0, wpb=404131.880, bsz=10861.840, num_updates=614, lr=0.000153592, gnorm=0.607, clip=0.000, oom=0.000, wall=251, train_wall=234
resetting loss stats
| epoch 002:    300 / 413 loss=8.270, nll_loss=7.332, ppl=161.09, wps=161276, ups=0, wpb=404410.250, bsz=10872.560, num_updates=714, lr=0.000178591, gnorm=0.596, clip=0.000, oom=0.000, wall=251, train_wall=234
resetting loss stats
| epoch 002:    400 / 413 loss=7.859, nll_loss=6.855, ppl=115.79, wps=163751, ups=0, wpb=403745.510, bsz=10834.970, num_updates=814, lr=0.00020359, gnorm=0.602, clip=0.000, oom=0.000, wall=247, train_wall=233
resetting loss stats
| epoch 002 | loss 7.651 | nll_loss 6.615 | ppl 98.01 | wps 162884 | ups 0 | wpb 401051.750 | bsz 10754.000 | num_updates 826 | lr 0.00020659 | gnorm 0.598 | clip 0.000 | oom 0.000 | wall 30 | train_wall 28
| epoch 002 | valid on 'valid' subset | loss 7.480 | nll_loss 6.359 | ppl 82.06 | num_updates 826 | best_loss 7.47979
| saved checkpoint model-save-dir/checkpoint2.pt (epoch 2 @ 826 updates) (writing took 4.870078802108765 seconds)
| epoch 003:    100 / 413 loss=7.466, nll_loss=6.401, ppl=84.51, wps=161320, ups=0, wpb=405394.594, bsz=10968.010, num_updates=927, lr=0.000231838, gnorm=0.636, clip=0.000, oom=0.000, wall=291, train_wall=265
resetting loss stats
| epoch 003:    200 / 413 loss=7.199, nll_loss=6.091, ppl=68.19, wps=160505, ups=0, wpb=403041.520, bsz=10872.400, num_updates=1027, lr=0.000256837, gnorm=0.667, clip=0.000, oom=0.000, wall=251, train_wall=235
resetting loss stats
| epoch 003:    300 / 413 loss=6.950, nll_loss=5.805, ppl=55.90, wps=160988, ups=0, wpb=404739.460, bsz=10875.680, num_updates=1127, lr=0.000281836, gnorm=0.725, clip=0.000, oom=0.000, wall=251, train_wall=235
resetting loss stats
| epoch 003:    400 / 413 loss=6.751, nll_loss=5.575, ppl=47.66, wps=160992, ups=0, wpb=404199.140, bsz=10909.360, num_updates=1227, lr=0.000306835, gnorm=0.669, clip=0.000, oom=0.000, wall=251, train_wall=235
resetting loss stats
| epoch 003 | loss 6.643 | nll_loss 5.450 | ppl 43.72 | wps 160041 | ups 0 | wpb 396185.417 | bsz 10602.000 | num_updates 1239 | lr 0.000309835 | gnorm 0.719 | clip 0.000 | oom 0.000 | wall 30 | train_wall 28
| epoch 003 | valid on 'valid' subset | loss 6.334 | nll_loss 5.010 | ppl 32.22 | num_updates 1239 | best_loss 6.33426
| saved checkpoint model-save-dir/checkpoint3.pt (epoch 3 @ 1239 updates) (writing took 5.137833118438721 seconds)
| epoch 004:    100 / 413 loss=6.515, nll_loss=5.303, ppl=39.48, wps=161092, ups=0, wpb=404364.436, bsz=10842.396, num_updates=1340, lr=0.000335083, gnorm=0.690, clip=0.000, oom=0.000, wall=291, train_wall=265
resetting loss stats
| epoch 004:    200 / 413 loss=6.290, nll_loss=5.044, ppl=32.99, wps=161850, ups=0, wpb=403928.100, bsz=10976.080, num_updates=1440, lr=0.000360082, gnorm=0.715, clip=0.000, oom=0.000, wall=250, train_wall=234
resetting loss stats
| epoch 004:    300 / 413 loss=6.095, nll_loss=4.819, ppl=28.23, wps=163393, ups=0, wpb=404709.080, bsz=10881.830, num_updates=1540, lr=0.000385081, gnorm=0.643, clip=0.000, oom=0.000, wall=248, train_wall=234
resetting loss stats
| epoch 004:    400 / 413 loss=5.915, nll_loss=4.614, ppl=24.50, wps=161034, ups=0, wpb=404160.320, bsz=10948.000, num_updates=1640, lr=0.00041008, gnorm=0.609, clip=0.000, oom=0.000, wall=251, train_wall=235
resetting loss stats
| epoch 004 | loss 5.847 | nll_loss 4.537 | ppl 23.21 | wps 162300 | ups 0 | wpb 398044.417 | bsz 10422.000 | num_updates 1652 | lr 0.000413079 | gnorm 0.582 | clip 0.000 | oom 0.000 | wall 29 | train_wall 28
| epoch 004 | valid on 'valid' subset | loss 5.615 | nll_loss 4.147 | ppl 17.71 | num_updates 1652 | best_loss 5.61487
| saved checkpoint model-save-dir/checkpoint4.pt (epoch 4 @ 1652 updates) (writing took 4.772693395614624 seconds)
| epoch 005:    100 / 413 loss=5.747, nll_loss=4.423, ppl=21.45, wps=160649, ups=0, wpb=403764.941, bsz=10909.327, num_updates=1753, lr=0.000438328, gnorm=0.574, clip=0.000, oom=0.000, wall=291, train_wall=265
resetting loss stats
| epoch 005:    200 / 413 loss=5.618, nll_loss=4.277, ppl=19.39, wps=160937, ups=0, wpb=403891.800, bsz=10886.390, num_updates=1853, lr=0.000463327, gnorm=0.547, clip=0.000, oom=0.000, wall=251, train_wall=235
resetting loss stats
| epoch 005:    300 / 413 loss=5.478, nll_loss=4.118, ppl=17.37, wps=161226, ups=0, wpb=404741.550, bsz=10938.480, num_updates=1953, lr=0.000488326, gnorm=0.518, clip=0.000, oom=0.000, wall=251, train_wall=235
resetting loss stats
| epoch 005:    400 / 413 loss=5.392, nll_loss=4.022, ppl=16.25, wps=163219, ups=0, wpb=404546.190, bsz=10889.120, num_updates=2053, lr=0.000513324, gnorm=0.486, clip=0.000, oom=0.000, wall=248, train_wall=234
resetting loss stats
| epoch 005 | loss 5.342 | nll_loss 3.966 | ppl 15.63 | wps 164573 | ups 0 | wpb 399906.500 | bsz 10624.667 | num_updates 2065 | lr 0.000516324 | gnorm 0.626 | clip 0.000 | oom 0.000 | wall 29 | train_wall 28
| epoch 005 | valid on 'valid' subset | loss 5.054 | nll_loss 3.533 | ppl 11.57 | num_updates 2065 | best_loss 5.05396
| saved checkpoint model-save-dir/checkpoint5.pt (epoch 5 @ 2065 updates) (writing took 4.346589088439941 seconds)
| epoch 006:    100 / 413 loss=5.277, nll_loss=3.894, ppl=14.86, wps=161193, ups=0, wpb=403927.010, bsz=10867.713, num_updates=2166, lr=0.000541573, gnorm=0.443, clip=0.000, oom=0.000, wall=290, train_wall=264
resetting loss stats
| epoch 006:    200 / 413 loss=5.198, nll_loss=3.806, ppl=13.98, wps=161555, ups=0, wpb=404776.330, bsz=10840.320, num_updates=2266, lr=0.000566572, gnorm=0.444, clip=0.000, oom=0.000, wall=251, train_wall=235
resetting loss stats
| epoch 006:    300 / 413 loss=5.132, nll_loss=3.731, ppl=13.28, wps=163361, ups=0, wpb=404201.670, bsz=10942.400, num_updates=2366, lr=0.00059157, gnorm=0.439, clip=0.000, oom=0.000, wall=247, train_wall=235
resetting loss stats
| epoch 006:    400 / 413 loss=5.055, nll_loss=3.646, ppl=12.52, wps=163205, ups=0, wpb=404172.240, bsz=10958.980, num_updates=2466, lr=0.000616569, gnorm=0.409, clip=0.000, oom=0.000, wall=248, train_wall=235
resetting loss stats
| epoch 006 | loss 5.053 | nll_loss 3.645 | ppl 12.51 | wps 162984 | ups 0 | wpb 398786.583 | bsz 10744.000 | num_updates 2478 | lr 0.000619569 | gnorm 0.393 | clip 0.000 | oom 0.000 | wall 29 | train_wall 28
| epoch 006 | valid on 'valid' subset | loss 4.674 | nll_loss 3.101 | ppl 8.58 | num_updates 2478 | best_loss 4.67386
| saved checkpoint model-save-dir/checkpoint6.pt (epoch 6 @ 2478 updates) (writing took 5.62847113609314 seconds)
| epoch 007:    100 / 413 loss=4.987, nll_loss=3.571, ppl=11.88, wps=160570, ups=0, wpb=403611.465, bsz=10942.990, num_updates=2579, lr=0.000644818, gnorm=0.398, clip=0.000, oom=0.000, wall=292, train_wall=265
resetting loss stats
| epoch 007:    200 / 413 loss=4.927, nll_loss=3.504, ppl=11.34, wps=161510, ups=0, wpb=404965.360, bsz=10819.910, num_updates=2679, lr=0.000669817, gnorm=0.385, clip=0.000, oom=0.000, wall=251, train_wall=235
resetting loss stats
| epoch 007:    300 / 413 loss=4.893, nll_loss=3.467, ppl=11.06, wps=160837, ups=0, wpb=403740.090, bsz=10880.880, num_updates=2779, lr=0.000694815, gnorm=0.378, clip=0.000, oom=0.000, wall=251, train_wall=235
resetting loss stats
| epoch 007:    400 / 413 loss=4.846, nll_loss=3.415, ppl=10.67, wps=161252, ups=0, wpb=404718.020, bsz=10961.040, num_updates=2879, lr=0.000719814, gnorm=0.369, clip=0.000, oom=0.000, wall=251, train_wall=235
resetting loss stats
| epoch 007 | loss 4.816 | nll_loss 3.382 | ppl 10.43 | wps 161123 | ups 0 | wpb 399165.500 | bsz 10776.000 | num_updates 2891 | lr 0.000722814 | gnorm 0.357 | clip 0.000 | oom 0.000 | wall 30 | train_wall 28
| epoch 007 | valid on 'valid' subset | loss 4.440 | nll_loss 2.855 | ppl 7.23 | num_updates 2891 | best_loss 4.43995
| saved checkpoint model-save-dir/checkpoint7.pt (epoch 7 @ 2891 updates) (writing took 4.4990880489349365 seconds)
| epoch 008:    100 / 413 loss=4.797, nll_loss=3.360, ppl=10.27, wps=160690, ups=0, wpb=403719.950, bsz=10836.832, num_updates=2992, lr=0.000748063, gnorm=0.357, clip=0.000, oom=0.000, wall=291, train_wall=265
resetting loss stats
| epoch 008:    200 / 413 loss=4.758, nll_loss=3.318, ppl=9.97, wps=163658, ups=0, wpb=404555.850, bsz=10913.360, num_updates=3092, lr=0.000773061, gnorm=0.361, clip=0.000, oom=0.000, wall=247, train_wall=234
resetting loss stats
| epoch 008:    300 / 413 loss=4.715, nll_loss=3.271, ppl=9.65, wps=163400, ups=0, wpb=404326.940, bsz=10911.670, num_updates=3192, lr=0.00079806, gnorm=0.346, clip=0.000, oom=0.000, wall=247, train_wall=234
resetting loss stats
| epoch 008:    400 / 413 loss=4.672, nll_loss=3.222, ppl=9.33, wps=163917, ups=0, wpb=404837.360, bsz=10926.100, num_updates=3292, lr=0.000823059, gnorm=0.328, clip=0.000, oom=0.000, wall=247, train_wall=234
resetting loss stats
| epoch 008 | loss 4.667 | nll_loss 3.218 | ppl 9.31 | wps 159126 | ups 0 | wpb 395780.083 | bsz 10925.333 | num_updates 3304 | lr 0.000826059 | gnorm 0.448 | clip 0.000 | oom 0.000 | wall 30 | train_wall 28
| epoch 008 | valid on 'valid' subset | loss 4.312 | nll_loss 2.731 | ppl 6.64 | num_updates 3304 | best_loss 4.31172
| saved checkpoint model-save-dir/checkpoint8.pt (epoch 8 @ 3304 updates) (writing took 5.213096618652344 seconds)
| epoch 009:    100 / 413 loss=4.636, nll_loss=3.183, ppl=9.08, wps=160784, ups=0, wpb=404039.762, bsz=10967.386, num_updates=3405, lr=0.000851307, gnorm=0.322, clip=0.000, oom=0.000, wall=292, train_wall=265
resetting loss stats
| epoch 009:    200 / 413 loss=4.608, nll_loss=3.152, ppl=8.89, wps=161079, ups=0, wpb=404678.580, bsz=10893.520, num_updates=3505, lr=0.000876306, gnorm=0.333, clip=0.000, oom=0.000, wall=251, train_wall=234
resetting loss stats
| epoch 009:    300 / 413 loss=4.594, nll_loss=3.138, ppl=8.80, wps=160955, ups=0, wpb=405452.930, bsz=10912.790, num_updates=3605, lr=0.000901305, gnorm=0.326, clip=0.000, oom=0.000, wall=252, train_wall=235
resetting loss stats
| epoch 009:    400 / 413 loss=4.565, nll_loss=3.105, ppl=8.61, wps=160711, ups=0, wpb=403074.930, bsz=10866.960, num_updates=3705, lr=0.000926304, gnorm=0.314, clip=0.000, oom=0.000, wall=251, train_wall=234
resetting loss stats
| epoch 009 | loss 4.582 | nll_loss 3.126 | ppl 8.73 | wps 160739 | ups 0 | wpb 397369.250 | bsz 10475.333 | num_updates 3717 | lr 0.000929304 | gnorm 0.383 | clip 0.000 | oom 0.000 | wall 30 | train_wall 28
| epoch 009 | valid on 'valid' subset | loss 4.158 | nll_loss 2.554 | ppl 5.87 | num_updates 3717 | best_loss 4.15846
| saved checkpoint model-save-dir/checkpoint9.pt (epoch 9 @ 3717 updates) (writing took 4.203198671340942 seconds)
| epoch 010:    100 / 413 loss=4.525, nll_loss=3.061, ppl=8.34, wps=161073, ups=0, wpb=404210.693, bsz=10965.851, num_updates=3818, lr=0.000954552, gnorm=0.324, clip=0.000, oom=0.000, wall=291, train_wall=264
resetting loss stats
| epoch 010:    200 / 413 loss=4.508, nll_loss=3.043, ppl=8.24, wps=161366, ups=0, wpb=404990.200, bsz=10904.660, num_updates=3918, lr=0.000979551, gnorm=0.304, clip=0.000, oom=0.000, wall=251, train_wall=235
resetting loss stats
| epoch 010:    300 / 413 loss=4.496, nll_loss=3.030, ppl=8.17, wps=160881, ups=0, wpb=403597.870, bsz=10951.920, num_updates=4018, lr=0.00100455, gnorm=0.324, clip=0.000, oom=0.000, wall=251, train_wall=235
resetting loss stats
| epoch 010:    400 / 413 loss=4.475, nll_loss=3.008, ppl=8.04, wps=161047, ups=0, wpb=404383.550, bsz=10757.200, num_updates=4118, lr=0.00102955, gnorm=0.301, clip=0.000, oom=0.000, wall=251, train_wall=235
resetting loss stats
| epoch 010 | loss 4.443 | nll_loss 2.971 | ppl 7.84 | wps 160291 | ups 0 | wpb 397887.417 | bsz 10984.000 | num_updates 4130 | lr 0.00103255 | gnorm 0.263 | clip 0.000 | oom 0.000 | wall 30 | train_wall 28
| epoch 010 | valid on 'valid' subset | loss 4.050 | nll_loss 2.476 | ppl 5.56 | num_updates 4130 | best_loss 4.05032
| saved checkpoint model-save-dir/checkpoint10.pt (epoch 10 @ 4130 updates) (writing took 4.851716041564941 seconds)
| epoch 011:    100 / 413 loss=4.432, nll_loss=2.959, ppl=7.78, wps=163045, ups=0, wpb=404622.832, bsz=10791.208, num_updates=4231, lr=0.0010578, gnorm=0.308, clip=0.000, oom=0.000, wall=288, train_wall=264
resetting loss stats
| epoch 011:    200 / 413 loss=4.424, nll_loss=2.951, ppl=7.73, wps=164298, ups=0, wpb=404408.920, bsz=10998.400, num_updates=4331, lr=0.0010828, gnorm=0.310, clip=0.000, oom=0.000, wall=246, train_wall=233
resetting loss stats
| epoch 011:    300 / 413 loss=4.410, nll_loss=2.936, ppl=7.66, wps=161412, ups=0, wpb=404040.190, bsz=10902.090, num_updates=4431, lr=0.00110779, gnorm=0.316, clip=0.000, oom=0.000, wall=250, train_wall=235
resetting loss stats
| epoch 011:    400 / 413 loss=4.412, nll_loss=2.939, ppl=7.67, wps=161020, ups=0, wpb=404059.880, bsz=10918.720, num_updates=4531, lr=0.00113279, gnorm=0.311, clip=0.000, oom=0.000, wall=251, train_wall=234
resetting loss stats
| epoch 011 | loss 4.340 | nll_loss 2.858 | ppl 7.25 | wps 161531 | ups 0 | wpb 398273.833 | bsz 10742.000 | num_updates 4543 | lr 0.00113579 | gnorm 0.225 | clip 0.000 | oom 0.000 | wall 30 | train_wall 28
| epoch 011 | valid on 'valid' subset | loss 3.973 | nll_loss 2.376 | ppl 5.19 | num_updates 4543 | best_loss 3.9728
| saved checkpoint model-save-dir/checkpoint11.pt (epoch 11 @ 4543 updates) (writing took 4.433974504470825 seconds)
| epoch 012:    100 / 413 loss=4.374, nll_loss=2.896, ppl=7.45, wps=161228, ups=0, wpb=404020.604, bsz=10845.297, num_updates=4644, lr=0.00116104, gnorm=0.316, clip=0.000, oom=0.000, wall=290, train_wall=264
resetting loss stats
| epoch 012:    200 / 413 loss=4.362, nll_loss=2.884, ppl=7.38, wps=162838, ups=0, wpb=405123.510, bsz=10937.920, num_updates=4744, lr=0.00118604, gnorm=0.299, clip=0.000, oom=0.000, wall=249, train_wall=234
resetting loss stats
| epoch 012:    300 / 413 loss=4.346, nll_loss=2.867, ppl=7.29, wps=161142, ups=0, wpb=404573.320, bsz=10897.440, num_updates=4844, lr=0.00121104, gnorm=0.312, clip=0.000, oom=0.000, wall=251, train_wall=235
resetting loss stats
| epoch 012:    400 / 413 loss=4.353, nll_loss=2.875, ppl=7.34, wps=160657, ups=0, wpb=403092.260, bsz=10952.080, num_updates=4944, lr=0.00123604, gnorm=0.301, clip=0.000, oom=0.000, wall=251, train_wall=235
resetting loss stats
| epoch 012 | loss 4.376 | nll_loss 2.900 | ppl 7.46 | wps 164041 | ups 0 | wpb 401008.417 | bsz 10551.500 | num_updates 4956 | lr 0.00123904 | gnorm 0.395 | clip 0.000 | oom 0.000 | wall 29 | train_wall 28
| epoch 012 | valid on 'valid' subset | loss 3.967 | nll_loss 2.367 | ppl 5.16 | num_updates 4956 | best_loss 3.96655
| saved checkpoint model-save-dir/checkpoint12.pt (epoch 12 @ 4956 updates) (writing took 4.808370351791382 seconds)
| epoch 013:    100 / 413 loss=4.323, nll_loss=2.841, ppl=7.16, wps=161155, ups=0, wpb=404730.168, bsz=10893.149, num_updates=5057, lr=0.00126429, gnorm=0.291, clip=0.000, oom=0.000, wall=291, train_wall=265
resetting loss stats
| epoch 013:    200 / 413 loss=4.307, nll_loss=2.824, ppl=7.08, wps=161063, ups=0, wpb=404200.880, bsz=10930.230, num_updates=5157, lr=0.00128929, gnorm=0.306, clip=0.000, oom=0.000, wall=251, train_wall=235
resetting loss stats
| epoch 013:    300 / 413 loss=4.324, nll_loss=2.843, ppl=7.18, wps=161018, ups=0, wpb=403826.940, bsz=10876.080, num_updates=5257, lr=0.00131428, gnorm=0.308, clip=0.000, oom=0.000, wall=251, train_wall=234
resetting loss stats
| epoch 013:    400 / 413 loss=4.289, nll_loss=2.805, ppl=6.99, wps=161033, ups=0, wpb=404211.140, bsz=10913.140, num_updates=5357, lr=0.00133928, gnorm=0.296, clip=0.000, oom=0.000, wall=251, train_wall=235
resetting loss stats
| epoch 013 | loss 4.272 | nll_loss 2.786 | ppl 6.90 | wps 161616 | ups 0 | wpb 399620.667 | bsz 10715.333 | num_updates 5369 | lr 0.00134228 | gnorm 0.382 | clip 0.000 | oom 0.000 | wall 30 | train_wall 28
| epoch 013 | valid on 'valid' subset | loss 3.881 | nll_loss 2.286 | ppl 4.88 | num_updates 5369 | best_loss 3.88063
| saved checkpoint model-save-dir/checkpoint13.pt (epoch 13 @ 5369 updates) (writing took 4.7291834354400635 seconds)
| epoch 014:    100 / 413 loss=4.264, nll_loss=2.776, ppl=6.85, wps=161128, ups=0, wpb=404576.931, bsz=10933.782, num_updates=5470, lr=0.00136753, gnorm=0.297, clip=0.000, oom=0.000, wall=291, train_wall=265
resetting loss stats
| epoch 014:    200 / 413 loss=4.286, nll_loss=2.802, ppl=6.97, wps=161553, ups=0, wpb=403909.750, bsz=10875.590, num_updates=5570, lr=0.00139253, gnorm=0.313, clip=0.000, oom=0.000, wall=250, train_wall=235
resetting loss stats
| epoch 014:    300 / 413 loss=4.267, nll_loss=2.781, ppl=6.87, wps=162313, ups=0, wpb=404016.980, bsz=10843.940, num_updates=5670, lr=0.00141753, gnorm=0.301, clip=0.000, oom=0.000, wall=249, train_wall=235
resetting loss stats
| epoch 014:    400 / 413 loss=4.265, nll_loss=2.779, ppl=6.87, wps=161174, ups=0, wpb=404235.030, bsz=10967.680, num_updates=5770, lr=0.00144253, gnorm=0.318, clip=0.000, oom=0.000, wall=251, train_wall=235
resetting loss stats
| epoch 014 | loss 4.243 | nll_loss 2.754 | ppl 6.75 | wps 161670 | ups 0 | wpb 401553.750 | bsz 10642.000 | num_updates 5782 | lr 0.00144553 | gnorm 0.226 | clip 0.000 | oom 0.000 | wall 30 | train_wall 28
| epoch 014 | valid on 'valid' subset | loss 3.838 | nll_loss 2.233 | ppl 4.70 | num_updates 5782 | best_loss 3.83824
| saved checkpoint model-save-dir/checkpoint14.pt (epoch 14 @ 5782 updates) (writing took 4.241642713546753 seconds)
| epoch 015:    100 / 413 loss=4.240, nll_loss=2.751, ppl=6.73, wps=162458, ups=0, wpb=404599.139, bsz=10814.970, num_updates=5883, lr=0.00147078, gnorm=0.308, clip=0.000, oom=0.000, wall=289, train_wall=264
resetting loss stats
| epoch 015:    200 / 413 loss=4.232, nll_loss=2.742, ppl=6.69, wps=160669, ups=0, wpb=403670.500, bsz=10916.170, num_updates=5983, lr=0.00149578, gnorm=0.299, clip=0.000, oom=0.000, wall=251, train_wall=235
resetting loss stats
| epoch 015:    300 / 413 loss=4.245, nll_loss=2.758, ppl=6.76, wps=161405, ups=0, wpb=404978.240, bsz=10978.800, num_updates=6083, lr=0.00152077, gnorm=0.310, clip=0.000, oom=0.000, wall=251, train_wall=235
resetting loss stats
| epoch 015:    400 / 413 loss=4.240, nll_loss=2.753, ppl=6.74, wps=162057, ups=0, wpb=403933.570, bsz=10897.120, num_updates=6183, lr=0.00154577, gnorm=0.309, clip=0.000, oom=0.000, wall=249, train_wall=234
resetting loss stats
| epoch 015 | loss 4.242 | nll_loss 2.756 | ppl 6.75 | wps 163924 | ups 0 | wpb 397862.250 | bsz 10768.000 | num_updates 6195 | lr 0.00154877 | gnorm 0.311 | clip 0.000 | oom 0.000 | wall 29 | train_wall 28
| epoch 015 | valid on 'valid' subset | loss 3.817 | nll_loss 2.208 | ppl 4.62 | num_updates 6195 | best_loss 3.81687
| saved checkpoint model-save-dir/checkpoint15.pt (epoch 15 @ 6195 updates) (writing took 4.391470193862915 seconds)
| epoch 016:    100 / 413 loss=4.224, nll_loss=2.735, ppl=6.66, wps=160738, ups=0, wpb=403578.010, bsz=10909.139, num_updates=6296, lr=0.00157402, gnorm=0.312, clip=0.000, oom=0.000, wall=290, train_wall=264
resetting loss stats
| epoch 016:    200 / 413 loss=4.211, nll_loss=2.720, ppl=6.59, wps=161094, ups=0, wpb=404498.700, bsz=10903.680, num_updates=6396, lr=0.00159902, gnorm=0.307, clip=0.000, oom=0.000, wall=251, train_wall=235
resetting loss stats
| epoch 016:    300 / 413 loss=4.217, nll_loss=2.728, ppl=6.62, wps=161303, ups=0, wpb=404252.600, bsz=10912.580, num_updates=6496, lr=0.00162402, gnorm=0.303, clip=0.000, oom=0.000, wall=251, train_wall=234
resetting loss stats
| epoch 016:    400 / 413 loss=4.205, nll_loss=2.714, ppl=6.56, wps=162132, ups=0, wpb=404719.970, bsz=10936.080, num_updates=6596, lr=0.00164902, gnorm=0.313, clip=0.000, oom=0.000, wall=250, train_wall=234
resetting loss stats
| epoch 016 | loss 4.237 | nll_loss 2.751 | ppl 6.73 | wps 165295 | ups 0 | wpb 399048.750 | bsz 10306.667 | num_updates 6608 | lr 0.00165202 | gnorm 0.314 | clip 0.000 | oom 0.000 | wall 29 | train_wall 27
| epoch 016 | valid on 'valid' subset | loss 3.812 | nll_loss 2.185 | ppl 4.55 | num_updates 6608 | best_loss 3.81181
| saved checkpoint model-save-dir/checkpoint16.pt (epoch 16 @ 6608 updates) (writing took 4.412952899932861 seconds)
| epoch 017:    100 / 413 loss=4.196, nll_loss=2.703, ppl=6.51, wps=160493, ups=0, wpb=404223.634, bsz=10920.238, num_updates=6709, lr=0.00167727, gnorm=0.307, clip=0.000, oom=0.000, wall=291, train_wall=265
resetting loss stats
| epoch 017:    200 / 413 loss=4.198, nll_loss=2.706, ppl=6.53, wps=161101, ups=0, wpb=404724.190, bsz=10932.150, num_updates=6809, lr=0.00170226, gnorm=0.299, clip=0.000, oom=0.000, wall=251, train_wall=235
resetting loss stats
| epoch 017:    300 / 413 loss=4.191, nll_loss=2.698, ppl=6.49, wps=160820, ups=0, wpb=404138.480, bsz=10823.920, num_updates=6909, lr=0.00172726, gnorm=0.309, clip=0.000, oom=0.000, wall=251, train_wall=235
resetting loss stats
| epoch 017:    400 / 413 loss=4.203, nll_loss=2.713, ppl=6.56, wps=160816, ups=0, wpb=403886.510, bsz=10913.700, num_updates=7009, lr=0.00175226, gnorm=0.306, clip=0.000, oom=0.000, wall=251, train_wall=235
resetting loss stats
| epoch 017 | loss 4.189 | nll_loss 2.697 | ppl 6.48 | wps 161257 | ups 0 | wpb 399632.167 | bsz 10901.333 | num_updates 7021 | lr 0.00175526 | gnorm 0.238 | clip 0.000 | oom 0.000 | wall 30 | train_wall 28
| epoch 017 | valid on 'valid' subset | loss 3.773 | nll_loss 2.186 | ppl 4.55 | num_updates 7021 | best_loss 3.77326
| saved checkpoint model-save-dir/checkpoint17.pt (epoch 17 @ 7021 updates) (writing took 3.9266233444213867 seconds)
| epoch 018:    100 / 413 loss=4.171, nll_loss=2.677, ppl=6.39, wps=161395, ups=0, wpb=405557.673, bsz=10890.931, num_updates=7122, lr=0.00178051, gnorm=0.306, clip=0.000, oom=0.000, wall=290, train_wall=265
resetting loss stats
| epoch 018:    200 / 413 loss=4.181, nll_loss=2.688, ppl=6.45, wps=161062, ups=0, wpb=404336.830, bsz=10988.800, num_updates=7222, lr=0.00180551, gnorm=0.314, clip=0.000, oom=0.000, wall=251, train_wall=235
resetting loss stats
| epoch 018:    300 / 413 loss=4.192, nll_loss=2.701, ppl=6.50, wps=160968, ups=0, wpb=402977.540, bsz=10797.940, num_updates=7322, lr=0.00183051, gnorm=0.301, clip=0.000, oom=0.000, wall=250, train_wall=234
resetting loss stats
| epoch 018:    400 / 413 loss=4.168, nll_loss=2.674, ppl=6.38, wps=161066, ups=0, wpb=404263.240, bsz=10952.870, num_updates=7422, lr=0.00185551, gnorm=0.300, clip=0.000, oom=0.000, wall=251, train_wall=234
resetting loss stats
| epoch 018 | loss 4.188 | nll_loss 2.697 | ppl 6.48 | wps 160074 | ups 0 | wpb 398167.083 | bsz 10566.000 | num_updates 7434 | lr 0.00185851 | gnorm 0.338 | clip 0.000 | oom 0.000 | wall 30 | train_wall 28
| epoch 018 | valid on 'valid' subset | loss 3.753 | nll_loss 2.156 | ppl 4.46 | num_updates 7434 | best_loss 3.75288
| saved checkpoint model-save-dir/checkpoint18.pt (epoch 18 @ 7434 updates) (writing took 4.7223851680755615 seconds)
| epoch 019:    100 / 413 loss=4.155, nll_loss=2.659, ppl=6.32, wps=161247, ups=0, wpb=404818.307, bsz=10888.634, num_updates=7535, lr=0.00188376, gnorm=0.308, clip=0.000, oom=0.000, wall=291, train_wall=265
resetting loss stats
| epoch 019:    200 / 413 loss=4.173, nll_loss=2.681, ppl=6.41, wps=160881, ups=0, wpb=404088.450, bsz=10947.030, num_updates=7635, lr=0.00190875, gnorm=0.307, clip=0.000, oom=0.000, wall=251, train_wall=235
resetting loss stats
| epoch 019:    300 / 413 loss=4.155, nll_loss=2.660, ppl=6.32, wps=161562, ups=0, wpb=405235.200, bsz=10894.080, num_updates=7735, lr=0.00193375, gnorm=0.307, clip=0.000, oom=0.000, wall=251, train_wall=235
resetting loss stats
| epoch 019:    400 / 413 loss=4.174, nll_loss=2.683, ppl=6.42, wps=160453, ups=0, wpb=403032.130, bsz=10879.540, num_updates=7835, lr=0.00195875, gnorm=0.303, clip=0.000, oom=0.000, wall=251, train_wall=235
resetting loss stats
| epoch 019 | loss 4.168 | nll_loss 2.675 | ppl 6.39 | wps 160866 | ups 0 | wpb 397905.333 | bsz 10743.333 | num_updates 7847 | lr 0.00196175 | gnorm 0.296 | clip 0.000 | oom 0.000 | wall 30 | train_wall 28
| epoch 019 | valid on 'valid' subset | loss 3.743 | nll_loss 2.142 | ppl 4.42 | num_updates 7847 | best_loss 3.74322
| saved checkpoint model-save-dir/checkpoint19.pt (epoch 19 @ 7847 updates) (writing took 4.766191005706787 seconds)
| epoch 020:    100 / 413 loss=4.139, nll_loss=2.642, ppl=6.24, wps=161062, ups=0, wpb=404889.851, bsz=10962.941, num_updates=7948, lr=0.001987, gnorm=0.304, clip=0.000, oom=0.000, wall=292, train_wall=265
resetting loss stats
| epoch 020:    200 / 413 loss=4.157, nll_loss=2.664, ppl=6.34, wps=160873, ups=0, wpb=403874.320, bsz=10903.440, num_updates=8048, lr=0.00199403, gnorm=0.306, clip=0.000, oom=0.000, wall=251, train_wall=235
resetting loss stats
| epoch 020:    300 / 413 loss=4.155, nll_loss=2.661, ppl=6.32, wps=160720, ups=0, wpb=404396.440, bsz=10876.240, num_updates=8148, lr=0.00198175, gnorm=0.305, clip=0.000, oom=0.000, wall=252, train_wall=235
resetting loss stats
| epoch 020:    400 / 413 loss=4.145, nll_loss=2.651, ppl=6.28, wps=160974, ups=0, wpb=403951.550, bsz=10828.560, num_updates=8248, lr=0.0019697, gnorm=0.296, clip=0.000, oom=0.000, wall=251, train_wall=235
resetting loss stats
| epoch 020 | loss 4.128 | nll_loss 2.633 | ppl 6.20 | wps 160586 | ups 0 | wpb 398415.417 | bsz 11054.667 | num_updates 8260 | lr 0.00196827 | gnorm 0.278 | clip 0.000 | oom 0.000 | wall 30 | train_wall 28
| epoch 020 | valid on 'valid' subset | loss 3.731 | nll_loss 2.130 | ppl 4.38 | num_updates 8260 | best_loss 3.73081
| saved checkpoint model-save-dir/checkpoint20.pt (epoch 20 @ 8260 updates) (writing took 4.9435975551605225 seconds)
| epoch 021:    100 / 413 loss=4.120, nll_loss=2.623, ppl=6.16, wps=160909, ups=0, wpb=403862.208, bsz=10906.139, num_updates=8361, lr=0.00195635, gnorm=0.286, clip=0.000, oom=0.000, wall=292, train_wall=265
resetting loss stats
| epoch 021:    200 / 413 loss=4.129, nll_loss=2.633, ppl=6.20, wps=164323, ups=0, wpb=403675.400, bsz=10932.240, num_updates=8461, lr=0.00194475, gnorm=0.297, clip=0.000, oom=0.000, wall=246, train_wall=233
resetting loss stats
| epoch 021:    300 / 413 loss=4.121, nll_loss=2.624, ppl=6.17, wps=161258, ups=0, wpb=405272.490, bsz=10907.350, num_updates=8561, lr=0.00193336, gnorm=0.282, clip=0.000, oom=0.000, wall=251, train_wall=235
resetting loss stats
| epoch 021:    400 / 413 loss=4.131, nll_loss=2.636, ppl=6.22, wps=162404, ups=0, wpb=404346.850, bsz=10889.300, num_updates=8661, lr=0.00192217, gnorm=0.285, clip=0.000, oom=0.000, wall=249, train_wall=235
resetting loss stats
| epoch 021 | loss 4.131 | nll_loss 2.637 | ppl 6.22 | wps 162383 | ups 0 | wpb 398127.833 | bsz 10527.333 | num_updates 8673 | lr 0.00192084 | gnorm 0.380 | clip 0.000 | oom 0.000 | wall 29 | train_wall 28
| epoch 021 | valid on 'valid' subset | loss 3.711 | nll_loss 2.101 | ppl 4.29 | num_updates 8673 | best_loss 3.71117
| saved checkpoint model-save-dir/checkpoint21.pt (epoch 21 @ 8673 updates) (writing took 5.030447006225586 seconds)
| epoch 022:    100 / 413 loss=4.102, nll_loss=2.603, ppl=6.08, wps=160824, ups=0, wpb=404824.723, bsz=10960.871, num_updates=8774, lr=0.00190975, gnorm=0.279, clip=0.000, oom=0.000, wall=292, train_wall=265
resetting loss stats
| epoch 022:    200 / 413 loss=4.103, nll_loss=2.604, ppl=6.08, wps=160485, ups=0, wpb=403583.450, bsz=10959.350, num_updates=8874, lr=0.00189896, gnorm=0.275, clip=0.000, oom=0.000, wall=251, train_wall=235
resetting loss stats
| epoch 022:    300 / 413 loss=4.101, nll_loss=2.603, ppl=6.08, wps=161249, ups=0, wpb=404398.220, bsz=10827.040, num_updates=8974, lr=0.00188835, gnorm=0.291, clip=0.000, oom=0.000, wall=251, train_wall=234
resetting loss stats
| epoch 022:    400 / 413 loss=4.106, nll_loss=2.609, ppl=6.10, wps=161165, ups=0, wpb=404581.310, bsz=10871.060, num_updates=9074, lr=0.00187791, gnorm=0.282, clip=0.000, oom=0.000, wall=251, train_wall=234
resetting loss stats
| epoch 022 | loss 4.098 | nll_loss 2.601 | ppl 6.07 | wps 158994 | ups 0 | wpb 396124.667 | bsz 10662.000 | num_updates 9086 | lr 0.00187667 | gnorm 0.298 | clip 0.000 | oom 0.000 | wall 30 | train_wall 28
| epoch 022 | valid on 'valid' subset | loss 3.685 | nll_loss 2.072 | ppl 4.21 | num_updates 9086 | best_loss 3.68498
| saved checkpoint model-save-dir/checkpoint22.pt (epoch 22 @ 9086 updates) (writing took 4.485476732254028 seconds)
| epoch 023:    100 / 413 loss=4.077, nll_loss=2.576, ppl=5.96, wps=161016, ups=0, wpb=404838.802, bsz=10890.614, num_updates=9187, lr=0.00186633, gnorm=0.280, clip=0.000, oom=0.000, wall=292, train_wall=265
resetting loss stats
| epoch 023:    200 / 413 loss=4.080, nll_loss=2.581, ppl=5.98, wps=160914, ups=0, wpb=404273.750, bsz=10879.920, num_updates=9287, lr=0.00185625, gnorm=0.273, clip=0.000, oom=0.000, wall=251, train_wall=235
resetting loss stats
| epoch 023:    300 / 413 loss=4.084, nll_loss=2.585, ppl=6.00, wps=163651, ups=0, wpb=404616.310, bsz=11007.040, num_updates=9387, lr=0.00184634, gnorm=0.270, clip=0.000, oom=0.000, wall=247, train_wall=233
resetting loss stats
| epoch 023:    400 / 413 loss=4.084, nll_loss=2.586, ppl=6.00, wps=161472, ups=0, wpb=403507.160, bsz=10840.730, num_updates=9487, lr=0.00183658, gnorm=0.270, clip=0.000, oom=0.000, wall=250, train_wall=234
resetting loss stats
| epoch 023 | loss 4.093 | nll_loss 2.596 | ppl 6.05 | wps 160563 | ups 0 | wpb 397387.500 | bsz 10668.000 | num_updates 9499 | lr 0.00183542 | gnorm 0.267 | clip 0.000 | oom 0.000 | wall 30 | train_wall 28
| epoch 023 | valid on 'valid' subset | loss 3.671 | nll_loss 2.061 | ppl 4.17 | num_updates 9499 | best_loss 3.67107
| saved checkpoint model-save-dir/checkpoint23.pt (epoch 23 @ 9499 updates) (writing took 5.2631916999816895 seconds)
| epoch 024:    100 / 413 loss=4.056, nll_loss=2.554, ppl=5.87, wps=161397, ups=0, wpb=404997.812, bsz=10922.455, num_updates=9600, lr=0.00182574, gnorm=0.274, clip=0.000, oom=0.000, wall=292, train_wall=265
resetting loss stats
| epoch 024:    200 / 413 loss=4.062, nll_loss=2.560, ppl=5.90, wps=161208, ups=0, wpb=403705.930, bsz=10974.240, num_updates=9700, lr=0.00181631, gnorm=0.262, clip=0.000, oom=0.000, wall=250, train_wall=234
resetting loss stats
| epoch 024:    300 / 413 loss=4.070, nll_loss=2.571, ppl=5.94, wps=161330, ups=0, wpb=404340.020, bsz=10843.520, num_updates=9800, lr=0.00180702, gnorm=0.258, clip=0.000, oom=0.000, wall=251, train_wall=235
resetting loss stats
| epoch 024:    400 / 413 loss=4.066, nll_loss=2.567, ppl=5.92, wps=160870, ups=0, wpb=403998.630, bsz=10863.210, num_updates=9900, lr=0.00179787, gnorm=0.270, clip=0.000, oom=0.000, wall=251, train_wall=235
resetting loss stats
| epoch 024 | loss 4.056 | nll_loss 2.555 | ppl 5.88 | wps 160687 | ups 0 | wpb 398987.833 | bsz 10789.333 | num_updates 9912 | lr 0.00179678 | gnorm 0.234 | clip 0.000 | oom 0.000 | wall 30 | train_wall 28
| epoch 024 | valid on 'valid' subset | loss 3.652 | nll_loss 2.047 | ppl 4.13 | num_updates 9912 | best_loss 3.65237
| saved checkpoint model-save-dir/checkpoint24.pt (epoch 24 @ 9912 updates) (writing took 4.56142520904541 seconds)
| epoch 025:    100 / 413 loss=4.042, nll_loss=2.539, ppl=5.81, wps=160915, ups=0, wpb=404026.178, bsz=10909.010, num_updates=10013, lr=0.00178769, gnorm=0.263, clip=0.000, oom=0.000, wall=291, train_wall=265
resetting loss stats
| epoch 025:    200 / 413 loss=4.051, nll_loss=2.550, ppl=5.85, wps=160890, ups=0, wpb=404053.680, bsz=10888.150, num_updates=10113, lr=0.00177883, gnorm=0.257, clip=0.000, oom=0.000, wall=251, train_wall=235
resetting loss stats
| epoch 025:    300 / 413 loss=4.043, nll_loss=2.540, ppl=5.82, wps=164516, ups=0, wpb=404313.780, bsz=10946.160, num_updates=10213, lr=0.0017701, gnorm=0.256, clip=0.000, oom=0.000, wall=246, train_wall=233
resetting loss stats
| epoch 025:    400 / 413 loss=4.044, nll_loss=2.543, ppl=5.83, wps=163273, ups=0, wpb=405090.710, bsz=10879.040, num_updates=10313, lr=0.0017615, gnorm=0.253, clip=0.000, oom=0.000, wall=248, train_wall=235
resetting loss stats
| epoch 025 | loss 4.079 | nll_loss 2.582 | ppl 5.99 | wps 161603 | ups 0 | wpb 395385.833 | bsz 10632.667 | num_updates 10325 | lr 0.00176048 | gnorm 0.262 | clip 0.000 | oom 0.000 | wall 29 | train_wall 28
| epoch 025 | valid on 'valid' subset | loss 3.641 | nll_loss 2.036 | ppl 4.10 | num_updates 10325 | best_loss 3.6413
| saved checkpoint model-save-dir/checkpoint25.pt (epoch 25 @ 10325 updates) (writing took 4.483715772628784 seconds)
| epoch 026:    100 / 413 loss=4.018, nll_loss=2.512, ppl=5.70, wps=161975, ups=0, wpb=404105.545, bsz=10926.248, num_updates=10426, lr=0.00175193, gnorm=0.262, clip=0.000, oom=0.000, wall=289, train_wall=266
resetting loss stats
| epoch 026:    200 / 413 loss=4.040, nll_loss=2.538, ppl=5.81, wps=162957, ups=0, wpb=404358.090, bsz=10863.200, num_updates=10526, lr=0.00174359, gnorm=0.255, clip=0.000, oom=0.000, wall=248, train_wall=236
resetting loss stats
| epoch 026:    300 / 413 loss=4.030, nll_loss=2.527, ppl=5.76, wps=161674, ups=0, wpb=404775.580, bsz=10951.140, num_updates=10626, lr=0.00173536, gnorm=0.251, clip=0.000, oom=0.000, wall=250, train_wall=235
resetting loss stats
| epoch 026:    400 / 413 loss=4.037, nll_loss=2.535, ppl=5.79, wps=164729, ups=0, wpb=404036.690, bsz=10850.720, num_updates=10726, lr=0.00172725, gnorm=0.261, clip=0.000, oom=0.000, wall=245, train_wall=233
resetting loss stats
| epoch 026 | loss 4.048 | nll_loss 2.548 | ppl 5.85 | wps 163798 | ups 0 | wpb 397116.250 | bsz 10890.000 | num_updates 10738 | lr 0.00172629 | gnorm 0.251 | clip 0.000 | oom 0.000 | wall 29 | train_wall 28
| epoch 026 | valid on 'valid' subset | loss 3.627 | nll_loss 2.019 | ppl 4.05 | num_updates 10738 | best_loss 3.62747
| saved checkpoint model-save-dir/checkpoint26.pt (epoch 26 @ 10738 updates) (writing took 4.331773519515991 seconds)
| epoch 027:    100 / 413 loss=4.016, nll_loss=2.511, ppl=5.70, wps=160864, ups=0, wpb=403036.792, bsz=10884.851, num_updates=10839, lr=0.00171823, gnorm=0.254, clip=0.000, oom=0.000, wall=290, train_wall=264
resetting loss stats
| epoch 027:    200 / 413 loss=4.022, nll_loss=2.518, ppl=5.73, wps=164366, ups=0, wpb=404540.500, bsz=10954.960, num_updates=10939, lr=0.00171035, gnorm=0.254, clip=0.000, oom=0.000, wall=246, train_wall=234
resetting loss stats
| epoch 027:    300 / 413 loss=4.013, nll_loss=2.509, ppl=5.69, wps=164613, ups=0, wpb=405025.580, bsz=10813.270, num_updates=11039, lr=0.00170259, gnorm=0.242, clip=0.000, oom=0.000, wall=246, train_wall=234
resetting loss stats
| epoch 027:    400 / 413 loss=4.017, nll_loss=2.513, ppl=5.71, wps=161650, ups=0, wpb=404451.690, bsz=10931.600, num_updates=11139, lr=0.00169493, gnorm=0.244, clip=0.000, oom=0.000, wall=250, train_wall=234
resetting loss stats
| epoch 027 | loss 4.022 | nll_loss 2.518 | ppl 5.73 | wps 160946 | ups 0 | wpb 399049.833 | bsz 10948.667 | num_updates 11151 | lr 0.00169402 | gnorm 0.267 | clip 0.000 | oom 0.000 | wall 30 | train_wall 28
| epoch 027 | valid on 'valid' subset | loss 3.633 | nll_loss 2.027 | ppl 4.07 | num_updates 11151 | best_loss 3.62747
| saved checkpoint model-save-dir/checkpoint27.pt (epoch 27 @ 11151 updates) (writing took 2.774614095687866 seconds)
| epoch 028:    100 / 413 loss=4.011, nll_loss=2.506, ppl=5.68, wps=163275, ups=0, wpb=404018.109, bsz=10878.327, num_updates=11252, lr=0.0016864, gnorm=0.240, clip=0.000, oom=0.000, wall=286, train_wall=264
resetting loss stats
| epoch 028:    200 / 413 loss=3.999, nll_loss=2.493, ppl=5.63, wps=160783, ups=0, wpb=404758.250, bsz=10964.000, num_updates=11352, lr=0.00167895, gnorm=0.255, clip=0.000, oom=0.000, wall=252, train_wall=235
slurmstepd: error: *** JOB 2486044 ON gpu12 CANCELLED AT 2024-04-26T16:53:55 DUE TO TIME LIMIT ***
